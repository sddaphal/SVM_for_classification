{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM for classification\n",
    "This implements SVM to solve binary classification problems.  \n",
    "We minimize hinge loss function with penalty term:  \n",
    "$$min \\sum_{i=1}^mmax(0,1-yi*(w^T*xi+b))+\\rho*(w^T*w)$$\n",
    "\n",
    "#### Notation\n",
    "w = [w1, w2, ...ï¼Œ wn]  \n",
    "xi = [\\\\(x^{(1)}\\\\), \\\\(x^{(2)}\\\\), ...\\\\(x^{(n)}\\\\),] where n is the feature dimention    \n",
    "yi = -1 or 1   \n",
    "Training set is {(X,Y)} where X = [x1,x2, ..., xm] and Y = [y1, y2 ...,ym] where m is the sample size.\n",
    "## Stochatic subgradient descent for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#define some vector operations\n",
    "def dot(x, y):\n",
    "    'dot-multiplication of two vectors'\n",
    "    return sum([a*b for a, b in zip(x,y)])\n",
    "    \n",
    "def multi(c, y):\n",
    "    'Constant multiplies vector'\n",
    "    return [c*a for a in y]\n",
    "    \n",
    "def subtract(x, y):\n",
    "    'subtraction of two vectors x-y'\n",
    "    return [a-b for a, b in zip(x,y)]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to get subgradient\n",
    "Note that the gradient is of n+1 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subgradient(w, b, rho, x, y):\n",
    "    'get subgradient of loss func = max(0,1-yi*(w^T*xi+b))+\\rho*(w^T*w)'\n",
    "    #Note SGD only choose one sample (x,y) to get subgradient\n",
    "    #x is a vector, y is value -1 or 1\n",
    "    if 1-y*(dot(w, x)+b)>0:\n",
    "        g = multi(-y,x)+2*multi(rho,w)\n",
    "        g.append(-y)\n",
    "    else:\n",
    "        g = 2*multi(rho,w)\n",
    "        g.append(0)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Stochastic subgradient descient method\n",
    "There are two stop strategies (modify e to select):\n",
    "* Stop at itaration N (set e = 0)\n",
    "* Stop when the l2-norm of parameter's difference is less than the threshold e or iteration N is reached(set e > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SSGD(w0, b0, rho, lr, N, X, Y, e = 0):\n",
    "    'get w, b, iteration; can choose to stop iteration at e threshold or run full iteration(set e=0)'\n",
    "    #lr: learning rate    \n",
    "    #N: maximum number of iterations\n",
    "    #e: threshold to check convergence\n",
    "    #TODO output learning curve\n",
    "    w = w0\n",
    "    b = b0\n",
    "    for i in range(N):\n",
    "        #randomly choose one sample\n",
    "        k = random.randint(0, len(Y)-1)\n",
    "        x = X[k]\n",
    "        y = Y[k]\n",
    "        #get subgradient\n",
    "        g = subgradient(w, b, rho, x, y)\n",
    "        #update w, b\n",
    "        w_old = w\n",
    "        b_old = b\n",
    "        w = subtract(w, multi(lr, g[:-1]))\n",
    "        b = b-lr*g[-1]\n",
    "        #get l2-norm of (w, b)'s difference\n",
    "        delta = subtract(w, w_old) + [b-b_old]\n",
    "        norm_delta = sum([x*x for x in delta])\n",
    "        if e != 0 and norm_delta<e:   #less than threshold\n",
    "            print('Converge after {} iterations with threshold {}'.format(i+1, e))\n",
    "            return w, b, i+1\n",
    "    if e != 0:\n",
    "        print('Fail to converge in {} iterations with threshold {}'.format(i+1, e))\n",
    "    return w, b, i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "### Data generation: 100 samples with 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [[random.uniform(0,3), random.uniform(0,3)] for i in range(100)]\n",
    "true_w = [1,2]\n",
    "true_b = -4.5\n",
    "Y = []\n",
    "for x in X:\n",
    "    if dot(true_w, x)+true_b>0:\n",
    "        Y.append(1)\n",
    "    else:\n",
    "        Y.append(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w0 = [0, 0]\n",
    "b0 = 0\n",
    "rho = 0.1\n",
    "lr = 0.01    \n",
    "N = 100000\n",
    "#e = 0.000001\n",
    "w, b, iteration = SSGD(w0, b0, rho, lr, N, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
